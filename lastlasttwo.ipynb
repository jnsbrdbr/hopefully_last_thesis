{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lastlasttwo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvq636mP+GOY5OUwrdk3CB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnsbrdbr/hopefully_last_thesis/blob/main/lastlasttwo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkAKdYCFLyja",
        "outputId": "34945735-8fed-4819-836d-d18892faa97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-rc0\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "iY24ihVDNKEV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import system packages\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import importlib\n",
        "\n",
        "#Import data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Importing ML/DL libraries\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc,precision_recall_fscore_support, average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix,accuracy_score\n",
        "\n",
        "from keras import initializers\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, BatchNormalization, LeakyReLU, Dense, Reshape, Flatten, Activation \n",
        "from keras.layers import Dropout, multiply, GaussianNoise, MaxPooling2D, concatenate\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "random.seed(123)"
      ],
      "metadata": {
        "id": "0Ffr49auNMK5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVV9rEFENR_g",
        "outputId": "742cc7bd-3761-4cd5-ac78-7c7152fb9b82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with pd.HDFStore('/content/drive/MyDrive/justin/total_result',  mode='r') as newstore:\n",
        "    result = newstore.select('result')"
      ],
      "metadata": {
        "id": "7SQKZzKENZwz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "0jNQ1AANNwyf",
        "outputId": "353d3162-aeec-495f-8084-559eac6ff5bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2             3             4         5    \\\n",
              "0   -1.715972 -3.099936 -0.041249 -9.847494e-15  2.814883e-14  0.077295   \n",
              "1   -1.715972 -3.099936 -0.041249 -1.896065e-14 -1.670765e-14  0.077295   \n",
              "2   -1.715972 -3.099936 -0.041249 -1.371470e-13 -1.120560e-13  0.077295   \n",
              "3   -1.715972 -3.099936 -0.041249  6.277415e-14 -1.657541e-13  0.077295   \n",
              "4   -1.715972 -3.099936 -0.041249 -3.701367e-14  2.129133e-13  0.077295   \n",
              "..        ...       ...       ...           ...           ...       ...   \n",
              "995 -2.114893 -4.572474 -0.754679  3.266507e-13 -3.329586e-13  8.797502   \n",
              "996 -2.114893 -4.572474 -0.754679  3.270113e-13 -3.336144e-13  8.797502   \n",
              "997 -2.114893 -4.572474 -0.754679  3.254696e-13 -3.330911e-13  8.797502   \n",
              "998 -2.114893 -4.572474 -0.754679  3.253531e-13 -3.329030e-13  8.797502   \n",
              "999 -2.114893 -4.572474 -0.754679  3.260101e-13 -3.333168e-13  8.797502   \n",
              "\n",
              "          6         7             8             9    ...        101  \\\n",
              "0    1.691428  3.341114  2.248145e-13 -1.892775e-14  ...   0.109748   \n",
              "1    1.691428  3.341114 -4.553240e-14 -7.798314e-14  ...   0.109748   \n",
              "2    1.691428  3.341114 -7.958873e-14 -2.202776e-13  ...   0.109748   \n",
              "3    1.691428  3.341114  6.961887e-14 -1.492418e-13  ...   0.109748   \n",
              "4    1.691428  3.341114  1.081081e-13  1.537838e-13  ... -11.935691   \n",
              "..        ...       ...           ...           ...  ...        ...   \n",
              "995 -0.661164 -0.063010  9.856194e-03 -8.536521e-03  ...   0.128642   \n",
              "996 -0.661164 -0.063010 -3.501650e-02 -6.797294e-02  ...   0.128642   \n",
              "997 -0.661164 -0.063010 -1.889261e-02 -1.737909e-02  ...   0.128642   \n",
              "998 -0.661164 -0.063010  3.132647e-02 -1.020799e-02  ...   0.128642   \n",
              "999 -0.661164 -0.063010 -1.710380e-03 -2.918516e-02  ...   0.128642   \n",
              "\n",
              "           102       103       104           105           106       107  \\\n",
              "0     0.187462 -0.926807  0.001555  2.603925e-11 -5.189091e-11 -0.002291   \n",
              "1     0.187462 -0.926807  0.001555 -9.481436e-12  2.031252e-11 -0.002291   \n",
              "2     0.187462 -0.926807  0.001555 -5.071955e-12  1.127633e-11 -0.002291   \n",
              "3     0.187462 -0.926807  0.001555 -1.244652e-11  2.576609e-11 -0.002291   \n",
              "4   -29.759005 -0.507027 -0.476180  6.215240e-13 -1.774149e-13  0.884093   \n",
              "..         ...       ...       ...           ...           ...       ...   \n",
              "995   0.210675 -1.647476  0.001097  6.209370e-13  3.990504e-13 -0.000867   \n",
              "996   0.210675 -1.647476  0.001097  6.204323e-13  4.001116e-13 -0.000867   \n",
              "997   0.210675 -1.647476  0.001097  6.208547e-13  3.993398e-13 -0.000867   \n",
              "998   0.210675 -1.647476  0.001097  6.207445e-13  3.997803e-13 -0.000867   \n",
              "999   0.210675 -1.647476  0.001097  6.206516e-13  3.999651e-13 -0.000867   \n",
              "\n",
              "          108       109  110  \n",
              "0   -1.817205 -0.021183    1  \n",
              "1   -1.817205 -0.021183    1  \n",
              "2   -1.817205 -0.021183    1  \n",
              "3   -1.817205 -0.021183    1  \n",
              "4   -1.749524 -0.183266    1  \n",
              "..        ...       ...  ...  \n",
              "995 -4.540979 -0.763937    1  \n",
              "996 -4.540979 -0.763937    1  \n",
              "997 -4.540979 -0.763937    1  \n",
              "998 -4.540979 -0.763937    1  \n",
              "999 -4.540979 -0.763937    1  \n",
              "\n",
              "[1000 rows x 111 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a742453a-0ebb-443c-a516-22892569dedb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-9.847494e-15</td>\n",
              "      <td>2.814883e-14</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>2.248145e-13</td>\n",
              "      <td>-1.892775e-14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>2.603925e-11</td>\n",
              "      <td>-5.189091e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-1.896065e-14</td>\n",
              "      <td>-1.670765e-14</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>-4.553240e-14</td>\n",
              "      <td>-7.798314e-14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-9.481436e-12</td>\n",
              "      <td>2.031252e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-1.371470e-13</td>\n",
              "      <td>-1.120560e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>-7.958873e-14</td>\n",
              "      <td>-2.202776e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-5.071955e-12</td>\n",
              "      <td>1.127633e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>6.277415e-14</td>\n",
              "      <td>-1.657541e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>6.961887e-14</td>\n",
              "      <td>-1.492418e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-1.244652e-11</td>\n",
              "      <td>2.576609e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-3.701367e-14</td>\n",
              "      <td>2.129133e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>1.081081e-13</td>\n",
              "      <td>1.537838e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.935691</td>\n",
              "      <td>-29.759005</td>\n",
              "      <td>-0.507027</td>\n",
              "      <td>-0.476180</td>\n",
              "      <td>6.215240e-13</td>\n",
              "      <td>-1.774149e-13</td>\n",
              "      <td>0.884093</td>\n",
              "      <td>-1.749524</td>\n",
              "      <td>-0.183266</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.266507e-13</td>\n",
              "      <td>-3.329586e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>9.856194e-03</td>\n",
              "      <td>-8.536521e-03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.209370e-13</td>\n",
              "      <td>3.990504e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.270113e-13</td>\n",
              "      <td>-3.336144e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-3.501650e-02</td>\n",
              "      <td>-6.797294e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.204323e-13</td>\n",
              "      <td>4.001116e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.254696e-13</td>\n",
              "      <td>-3.330911e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-1.889261e-02</td>\n",
              "      <td>-1.737909e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.208547e-13</td>\n",
              "      <td>3.993398e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.253531e-13</td>\n",
              "      <td>-3.329030e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>3.132647e-02</td>\n",
              "      <td>-1.020799e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.207445e-13</td>\n",
              "      <td>3.997803e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.260101e-13</td>\n",
              "      <td>-3.333168e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-1.710380e-03</td>\n",
              "      <td>-2.918516e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.206516e-13</td>\n",
              "      <td>3.999651e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 111 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a742453a-0ebb-443c-a516-22892569dedb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a742453a-0ebb-443c-a516-22892569dedb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a742453a-0ebb-443c-a516-22892569dedb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.rename(columns = {110:'label'}, inplace = True)"
      ],
      "metadata": {
        "id": "MzIsDIA8QeSn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "UzBiu84iQjVZ",
        "outputId": "f442d8c7-3c83-4854-e346-e669b13e229a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2             3             4         5  \\\n",
              "0   -1.715972 -3.099936 -0.041249 -9.847494e-15  2.814883e-14  0.077295   \n",
              "1   -1.715972 -3.099936 -0.041249 -1.896065e-14 -1.670765e-14  0.077295   \n",
              "2   -1.715972 -3.099936 -0.041249 -1.371470e-13 -1.120560e-13  0.077295   \n",
              "3   -1.715972 -3.099936 -0.041249  6.277415e-14 -1.657541e-13  0.077295   \n",
              "4   -1.715972 -3.099936 -0.041249 -3.701367e-14  2.129133e-13  0.077295   \n",
              "..        ...       ...       ...           ...           ...       ...   \n",
              "995 -2.114893 -4.572474 -0.754679  3.266507e-13 -3.329586e-13  8.797502   \n",
              "996 -2.114893 -4.572474 -0.754679  3.270113e-13 -3.336144e-13  8.797502   \n",
              "997 -2.114893 -4.572474 -0.754679  3.254696e-13 -3.330911e-13  8.797502   \n",
              "998 -2.114893 -4.572474 -0.754679  3.253531e-13 -3.329030e-13  8.797502   \n",
              "999 -2.114893 -4.572474 -0.754679  3.260101e-13 -3.333168e-13  8.797502   \n",
              "\n",
              "            6         7             8             9  ...        101  \\\n",
              "0    1.691428  3.341114  2.248145e-13 -1.892775e-14  ...   0.109748   \n",
              "1    1.691428  3.341114 -4.553240e-14 -7.798314e-14  ...   0.109748   \n",
              "2    1.691428  3.341114 -7.958873e-14 -2.202776e-13  ...   0.109748   \n",
              "3    1.691428  3.341114  6.961887e-14 -1.492418e-13  ...   0.109748   \n",
              "4    1.691428  3.341114  1.081081e-13  1.537838e-13  ... -11.935691   \n",
              "..        ...       ...           ...           ...  ...        ...   \n",
              "995 -0.661164 -0.063010  9.856194e-03 -8.536521e-03  ...   0.128642   \n",
              "996 -0.661164 -0.063010 -3.501650e-02 -6.797294e-02  ...   0.128642   \n",
              "997 -0.661164 -0.063010 -1.889261e-02 -1.737909e-02  ...   0.128642   \n",
              "998 -0.661164 -0.063010  3.132647e-02 -1.020799e-02  ...   0.128642   \n",
              "999 -0.661164 -0.063010 -1.710380e-03 -2.918516e-02  ...   0.128642   \n",
              "\n",
              "           102       103       104           105           106       107  \\\n",
              "0     0.187462 -0.926807  0.001555  2.603925e-11 -5.189091e-11 -0.002291   \n",
              "1     0.187462 -0.926807  0.001555 -9.481436e-12  2.031252e-11 -0.002291   \n",
              "2     0.187462 -0.926807  0.001555 -5.071955e-12  1.127633e-11 -0.002291   \n",
              "3     0.187462 -0.926807  0.001555 -1.244652e-11  2.576609e-11 -0.002291   \n",
              "4   -29.759005 -0.507027 -0.476180  6.215240e-13 -1.774149e-13  0.884093   \n",
              "..         ...       ...       ...           ...           ...       ...   \n",
              "995   0.210675 -1.647476  0.001097  6.209370e-13  3.990504e-13 -0.000867   \n",
              "996   0.210675 -1.647476  0.001097  6.204323e-13  4.001116e-13 -0.000867   \n",
              "997   0.210675 -1.647476  0.001097  6.208547e-13  3.993398e-13 -0.000867   \n",
              "998   0.210675 -1.647476  0.001097  6.207445e-13  3.997803e-13 -0.000867   \n",
              "999   0.210675 -1.647476  0.001097  6.206516e-13  3.999651e-13 -0.000867   \n",
              "\n",
              "          108       109  label  \n",
              "0   -1.817205 -0.021183      1  \n",
              "1   -1.817205 -0.021183      1  \n",
              "2   -1.817205 -0.021183      1  \n",
              "3   -1.817205 -0.021183      1  \n",
              "4   -1.749524 -0.183266      1  \n",
              "..        ...       ...    ...  \n",
              "995 -4.540979 -0.763937      1  \n",
              "996 -4.540979 -0.763937      1  \n",
              "997 -4.540979 -0.763937      1  \n",
              "998 -4.540979 -0.763937      1  \n",
              "999 -4.540979 -0.763937      1  \n",
              "\n",
              "[1000 rows x 111 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-953efaf5-d455-416c-aff4-8b4006a50f2d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-9.847494e-15</td>\n",
              "      <td>2.814883e-14</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>2.248145e-13</td>\n",
              "      <td>-1.892775e-14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>2.603925e-11</td>\n",
              "      <td>-5.189091e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-1.896065e-14</td>\n",
              "      <td>-1.670765e-14</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>-4.553240e-14</td>\n",
              "      <td>-7.798314e-14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-9.481436e-12</td>\n",
              "      <td>2.031252e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-1.371470e-13</td>\n",
              "      <td>-1.120560e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>-7.958873e-14</td>\n",
              "      <td>-2.202776e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-5.071955e-12</td>\n",
              "      <td>1.127633e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>6.277415e-14</td>\n",
              "      <td>-1.657541e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>6.961887e-14</td>\n",
              "      <td>-1.492418e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.109748</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>-0.926807</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>-1.244652e-11</td>\n",
              "      <td>2.576609e-11</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>-1.817205</td>\n",
              "      <td>-0.021183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.715972</td>\n",
              "      <td>-3.099936</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>-3.701367e-14</td>\n",
              "      <td>2.129133e-13</td>\n",
              "      <td>0.077295</td>\n",
              "      <td>1.691428</td>\n",
              "      <td>3.341114</td>\n",
              "      <td>1.081081e-13</td>\n",
              "      <td>1.537838e-13</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.935691</td>\n",
              "      <td>-29.759005</td>\n",
              "      <td>-0.507027</td>\n",
              "      <td>-0.476180</td>\n",
              "      <td>6.215240e-13</td>\n",
              "      <td>-1.774149e-13</td>\n",
              "      <td>0.884093</td>\n",
              "      <td>-1.749524</td>\n",
              "      <td>-0.183266</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.266507e-13</td>\n",
              "      <td>-3.329586e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>9.856194e-03</td>\n",
              "      <td>-8.536521e-03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.209370e-13</td>\n",
              "      <td>3.990504e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.270113e-13</td>\n",
              "      <td>-3.336144e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-3.501650e-02</td>\n",
              "      <td>-6.797294e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.204323e-13</td>\n",
              "      <td>4.001116e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.254696e-13</td>\n",
              "      <td>-3.330911e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-1.889261e-02</td>\n",
              "      <td>-1.737909e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.208547e-13</td>\n",
              "      <td>3.993398e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.253531e-13</td>\n",
              "      <td>-3.329030e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>3.132647e-02</td>\n",
              "      <td>-1.020799e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.207445e-13</td>\n",
              "      <td>3.997803e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-2.114893</td>\n",
              "      <td>-4.572474</td>\n",
              "      <td>-0.754679</td>\n",
              "      <td>3.260101e-13</td>\n",
              "      <td>-3.333168e-13</td>\n",
              "      <td>8.797502</td>\n",
              "      <td>-0.661164</td>\n",
              "      <td>-0.063010</td>\n",
              "      <td>-1.710380e-03</td>\n",
              "      <td>-2.918516e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128642</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-1.647476</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>6.206516e-13</td>\n",
              "      <td>3.999651e-13</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-4.540979</td>\n",
              "      <td>-0.763937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 111 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-953efaf5-d455-416c-aff4-8b4006a50f2d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-953efaf5-d455-416c-aff4-8b4006a50f2d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-953efaf5-d455-416c-aff4-8b4006a50f2d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#divide data in to data_label and non_data label\n",
        "\n",
        "y=result.label\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "for i in range(len(y)):\n",
        "  if y[i]==1:\n",
        "    y[i]=0\n",
        "  else:\n",
        "    y[i]=1\n",
        "\n",
        "result_nonlabel=copy.deepcopy(result)\n",
        "del result_nonlabel['label']\n",
        "X=result_nonlabel"
      ],
      "metadata": {
        "id": "Fnp_ZeqFQl7Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data in to test and train...\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, x_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,shuffle=True,random_state=1)#random_state=42,"
      ],
      "metadata": {
        "id": "HleQaaYDQvia"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of fake in y\n",
        "print('Number of fake in y:',collections.Counter(y)[1])\n",
        "\n",
        "#number of real in y\n",
        "print('Number of real in y:',collections.Counter(y)[0])\n",
        "\n",
        "#number of fake in y_train\n",
        "print('Number of fake in y_train:',collections.Counter(y_train)[1])\n",
        "\n",
        "#number of real in y_train\n",
        "print('Number of real in y_train:',collections.Counter(y_train)[0])\n",
        "\n",
        "#number of fake in y_test\n",
        "print('Number of fake in y_test:',collections.Counter(y_test)[1])\n",
        "\n",
        "#number of real in y_test\n",
        "print('Number of real in y_test:',collections.Counter(y_test)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vuGJ5efQzla",
        "outputId": "8380e8d5-9d7b-454c-ff32-ce872a901c3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fake in y: 10\n",
            "Number of real in y: 990\n",
            "Number of fake in y_train: 7\n",
            "Number of real in y_train: 693\n",
            "Number of fake in y_test: 3\n",
            "Number of real in y_test: 297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Subsetting only Normal Network packets in our training set\n",
        "\n",
        "temp_df = X_train.copy()\n",
        "temp_df['label'] = y_train\n",
        "temp_df = temp_df.loc[temp_df['label'] == 0]\n",
        "temp_df = temp_df.drop('label', axis = 1)\n",
        "x_train = temp_df.copy()"
      ],
      "metadata": {
        "id": "YbSurJDPQ85q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the above splits using the MinMaxScaler from the scikit learn package\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Make sure to only fit the scaler on the training data\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "#Creating dataset dictionary \n",
        "dataset = {}\n",
        "dataset['x_train'] = x_train.astype(np.float32)\n",
        "dataset['y_train'] = y_train.astype(np.float32)\n",
        "dataset['x_test']  = x_test.astype(np.float32)\n",
        "dataset['y_test']  = y_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "x-XMnx_3RA6Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many anomalies are in our Testing set\n",
        "print('Number of Normal Network packets in the Training set:', x_train.shape[0])\n",
        "print('Number of Normal Network packets in the Testing set:', collections.Counter(y_test)[0])\n",
        "print('Number of Anomalous Network packets in the Testing set:', collections.Counter(y_test)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAPWu72SRGoW",
        "outputId": "675f642e-41a4-4547-cedc-ebaa5ee02a3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Normal Network packets in the Training set: 693\n",
            "Number of Normal Network packets in the Testing set: 297\n",
            "Number of Anomalous Network packets in the Testing set: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Generator network\n",
        "\n",
        "def get_generator(optimizer):\n",
        "    \n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(64, input_dim=110, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
        "    generator.add(Activation('tanh'))\n",
        "    \n",
        "    #generator.add(Dense(64))\n",
        "    #generator.add(Activation('tanh'))\n",
        "    \n",
        "    #generator.add(Dense(128))\n",
        "    #generator.add(Activation('tanh'))\n",
        "    \n",
        "    #generator.add(Dense(128))\n",
        "    #generator.add(Activation('tanh'))\n",
        "       \n",
        "    #generator.add(Dense(256))\n",
        "    #generator.add(Activation('tanh'))\n",
        "\n",
        "    #generator.add(Dense(256))\n",
        "    #generator.add(Activation('tanh'))\n",
        "   \n",
        "    generator.add(Dense(110, activation='tanh'))\n",
        "    \n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return generator"
      ],
      "metadata": {
        "id": "B8fauGwOS0NR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Defining the Discriminator network\n",
        "\n",
        "def get_discriminator(optimizer):\n",
        "    \n",
        "    discriminator = Sequential()\n",
        "    \n",
        "    discriminator.add(Dense(256, input_dim=110, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "       \n",
        "    discriminator.add(Dense(128))\n",
        "    discriminator.add(Activation('relu'))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "    \n",
        "    #discriminator.add(Dense(128))\n",
        "    #discriminator.add(Activation('relu'))\n",
        "    #discriminator.add(Dropout(0.2))\n",
        "\n",
        "    #discriminator.add(Dense(128))\n",
        "    #discriminator.add(Activation('relu'))\n",
        "    #discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(1))\n",
        "    discriminator.add(Activation('sigmoid'))\n",
        "   \n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return discriminator"
      ],
      "metadata": {
        "id": "6OG5jhIiTIDA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gan_network(discriminator, generator, optimizer,input_dim=110):\n",
        "\n",
        "    discriminator.trainable = False   \n",
        "    gan_input = Input(shape=(input_dim,))  \n",
        "    x = generator(gan_input)        \n",
        "    gan_output = discriminator(x)\n",
        "    \n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)    \n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return gan"
      ],
      "metadata": {
        "id": "0DCf1mSGTPTc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqQ6iry2TSkZ",
        "outputId": "5f2301e6-7f46-4068-e4f1-24f98567eb61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.00001\n",
        "batch_size = 16\n",
        "epochs = 100\n",
        "adam = Adam(lr = learning_rate,beta_1 = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPwNGDITXeJ",
        "outputId": "491b091e-38d9-40a6-d6f8-448eb1ad9e52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the GAN\n",
        "x_train, y_train, x_test, y_test = dataset['x_train'], dataset['y_train'],dataset['x_test'],dataset['y_test']\n",
        "\n",
        "#Calculating the number of batches based on the batch size\n",
        "batch_count = x_train.shape[0] // batch_size\n",
        "pbar = tqdm(total=epochs * batch_count)\n",
        "gan_loss = []\n",
        "discriminator_loss = []\n",
        "\n",
        "#Inititalizing the network\n",
        "generator = get_generator(adam)\n",
        "discriminator = get_discriminator(adam)\n",
        "gan = get_gan_network(discriminator, generator, adam,input_dim=110)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):        \n",
        "    for index in range(batch_count):        \n",
        "        pbar.update(1)        \n",
        "        # Creating a random set of input noise and images\n",
        "        noise = np.random.normal(0, 1, size=[batch_size,110])\n",
        "        \n",
        "        # Generate fake samples\n",
        "        generated_images = generator.predict_on_batch(noise)\n",
        "        \n",
        "        #Obtain a batch of normal network packets\n",
        "        image_batch = x_train[index * batch_size: (index + 1) * batch_size]\n",
        "            \n",
        "        X = np.vstack((generated_images,image_batch))       \n",
        "        y_dis = np.ones(2*batch_size) \n",
        "        y_dis[:batch_size] = 0\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator.trainable = True\n",
        "        d_loss= discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "        # Train generator\n",
        "        noise = np.random.uniform(0, 1, size=[batch_size, 110])\n",
        "        y_gen = np.ones(batch_size)\n",
        "        discriminator.trainable = False\n",
        "        g_loss = gan.train_on_batch(noise, y_gen)\n",
        "        \n",
        "        #Record the losses\n",
        "        discriminator_loss.append(d_loss)\n",
        "        gan_loss.append(g_loss)\n",
        "        \n",
        "    print(\"Epoch %d Batch %d/%d [D loss: %f] [G loss:%f]\" % (epoch,index,batch_count, d_loss, g_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2omqf0UThez",
        "outputId": "069f1f69-43d9-4883-b0b8-232bddcfb36f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 47/4300 [00:05<03:12, 22.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Batch 42/43 [D loss: 0.676431] [G loss:0.672164]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 89/4300 [00:07<03:26, 20.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 42/43 [D loss: 0.653460] [G loss:0.697439]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 131/4300 [00:09<03:29, 19.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Batch 42/43 [D loss: 0.603071] [G loss:0.718109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 174/4300 [00:11<03:33, 19.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Batch 42/43 [D loss: 0.587000] [G loss:0.689519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 219/4300 [00:14<03:33, 19.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Batch 42/43 [D loss: 0.557736] [G loss:0.663918]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 261/4300 [00:17<04:41, 14.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Batch 42/43 [D loss: 0.517373] [G loss:0.720479]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 305/4300 [00:19<03:48, 17.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Batch 42/43 [D loss: 0.484691] [G loss:0.705411]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 347/4300 [00:21<03:11, 20.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Batch 42/43 [D loss: 0.462767] [G loss:0.665097]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 390/4300 [00:23<03:21, 19.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Batch 42/43 [D loss: 0.406054] [G loss:0.642700]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 435/4300 [00:25<03:26, 18.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Batch 42/43 [D loss: 0.401614] [G loss:0.606843]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 480/4300 [00:28<02:29, 25.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Batch 42/43 [D loss: 0.380306] [G loss:0.534449]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 526/4300 [00:29<01:19, 47.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Batch 42/43 [D loss: 0.389332] [G loss:0.475785]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 565/4300 [00:29<01:14, 49.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Batch 42/43 [D loss: 0.315011] [G loss:0.441734]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 608/4300 [00:30<01:18, 46.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Batch 42/43 [D loss: 0.310764] [G loss:0.395012]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 653/4300 [00:31<01:17, 46.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Batch 42/43 [D loss: 0.278133] [G loss:0.350678]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 694/4300 [00:32<01:12, 49.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Batch 42/43 [D loss: 0.215914] [G loss:0.298056]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 737/4300 [00:33<01:12, 48.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Batch 42/43 [D loss: 0.220858] [G loss:0.203335]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 781/4300 [00:34<01:12, 48.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Batch 42/43 [D loss: 0.197841] [G loss:0.199398]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 825/4300 [00:35<01:09, 50.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Batch 42/43 [D loss: 0.202090] [G loss:0.187690]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 867/4300 [00:36<01:07, 51.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Batch 42/43 [D loss: 0.193157] [G loss:0.142922]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 912/4300 [00:37<01:12, 46.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Batch 42/43 [D loss: 0.194572] [G loss:0.115563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 954/4300 [00:37<01:08, 48.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Batch 42/43 [D loss: 0.128132] [G loss:0.112602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 994/4300 [00:38<01:20, 40.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 Batch 42/43 [D loss: 0.147416] [G loss:0.078280]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 1039/4300 [00:39<01:17, 42.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 Batch 42/43 [D loss: 0.146359] [G loss:0.060141]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1082/4300 [00:40<01:08, 47.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 Batch 42/43 [D loss: 0.094785] [G loss:0.053651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 1126/4300 [00:41<01:04, 49.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 Batch 42/43 [D loss: 0.084159] [G loss:0.058326]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 1169/4300 [00:42<01:08, 45.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 Batch 42/43 [D loss: 0.114384] [G loss:0.049325]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 1209/4300 [00:43<01:10, 43.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 Batch 42/43 [D loss: 0.088530] [G loss:0.061729]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 1257/4300 [00:44<01:01, 49.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 Batch 42/43 [D loss: 0.095388] [G loss:0.045036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 1299/4300 [00:45<01:01, 49.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 Batch 42/43 [D loss: 0.080911] [G loss:0.063653]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 1341/4300 [00:46<01:05, 45.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 Batch 42/43 [D loss: 0.054384] [G loss:0.030640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 1386/4300 [00:47<01:03, 45.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 Batch 42/43 [D loss: 0.069687] [G loss:0.028372]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1430/4300 [00:48<00:57, 50.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 Batch 42/43 [D loss: 0.064320] [G loss:0.033863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 1471/4300 [00:49<01:02, 45.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 Batch 42/43 [D loss: 0.047869] [G loss:0.042596]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 1511/4300 [00:50<01:02, 44.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 Batch 42/43 [D loss: 0.035479] [G loss:0.021498]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 1558/4300 [00:51<00:59, 46.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 Batch 42/43 [D loss: 0.036742] [G loss:0.035850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 1599/4300 [00:51<00:57, 47.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 Batch 42/43 [D loss: 0.058843] [G loss:0.020546]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 1641/4300 [00:52<00:58, 45.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 Batch 42/43 [D loss: 0.057630] [G loss:0.028790]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 1687/4300 [00:53<00:56, 46.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 Batch 42/43 [D loss: 0.079881] [G loss:0.009871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 1727/4300 [00:54<00:56, 45.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 Batch 42/43 [D loss: 0.040997] [G loss:0.015535]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 1769/4300 [00:55<01:08, 36.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 Batch 42/43 [D loss: 0.067027] [G loss:0.028276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 1815/4300 [00:56<00:55, 44.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Batch 42/43 [D loss: 0.041204] [G loss:0.023850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 1858/4300 [00:57<00:57, 42.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 Batch 42/43 [D loss: 0.033658] [G loss:0.008609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 1898/4300 [00:58<00:56, 42.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 Batch 42/43 [D loss: 0.041561] [G loss:0.012041]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 1944/4300 [00:59<00:52, 44.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 Batch 42/43 [D loss: 0.020764] [G loss:0.015119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 1986/4300 [01:00<00:48, 47.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 Batch 42/43 [D loss: 0.020100] [G loss:0.008507]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 2027/4300 [01:01<00:56, 40.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 Batch 42/43 [D loss: 0.030962] [G loss:0.005706]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 2070/4300 [01:02<00:52, 42.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 Batch 42/43 [D loss: 0.034071] [G loss:0.017456]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 2113/4300 [01:03<00:50, 43.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 Batch 42/43 [D loss: 0.028655] [G loss:0.004185]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2158/4300 [01:04<00:51, 41.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 Batch 42/43 [D loss: 0.029874] [G loss:0.013049]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 2203/4300 [01:05<00:45, 46.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 Batch 42/43 [D loss: 0.018756] [G loss:0.015839]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 2244/4300 [01:06<00:43, 47.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51 Batch 42/43 [D loss: 0.015678] [G loss:0.012031]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 2289/4300 [01:07<00:44, 45.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52 Batch 42/43 [D loss: 0.019100] [G loss:0.011819]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 2330/4300 [01:08<00:43, 45.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53 Batch 42/43 [D loss: 0.015505] [G loss:0.010089]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 2371/4300 [01:09<00:40, 47.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54 Batch 42/43 [D loss: 0.010623] [G loss:0.010355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 2413/4300 [01:10<00:42, 44.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55 Batch 42/43 [D loss: 0.018284] [G loss:0.004271]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 2458/4300 [01:11<00:44, 41.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56 Batch 42/43 [D loss: 0.021985] [G loss:0.007147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 2504/4300 [01:12<00:38, 46.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57 Batch 42/43 [D loss: 0.018432] [G loss:0.003150]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 2545/4300 [01:13<00:38, 45.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58 Batch 42/43 [D loss: 0.014934] [G loss:0.002421]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 2590/4300 [01:14<00:36, 46.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59 Batch 42/43 [D loss: 0.026683] [G loss:0.003577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 2632/4300 [01:15<00:35, 47.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60 Batch 42/43 [D loss: 0.011260] [G loss:0.005968]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 2673/4300 [01:16<00:35, 45.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61 Batch 42/43 [D loss: 0.016471] [G loss:0.005856]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 2718/4300 [01:17<00:35, 45.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62 Batch 42/43 [D loss: 0.020011] [G loss:0.004506]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 2759/4300 [01:18<00:31, 48.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63 Batch 42/43 [D loss: 0.010070] [G loss:0.001787]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 2805/4300 [01:18<00:30, 48.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64 Batch 42/43 [D loss: 0.009761] [G loss:0.003601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 2847/4300 [01:19<00:30, 47.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65 Batch 42/43 [D loss: 0.018794] [G loss:0.003783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 2887/4300 [01:20<00:28, 49.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66 Batch 42/43 [D loss: 0.010287] [G loss:0.002263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 2930/4300 [01:21<00:28, 48.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67 Batch 42/43 [D loss: 0.016719] [G loss:0.006094]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 2971/4300 [01:22<00:30, 43.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68 Batch 42/43 [D loss: 0.009759] [G loss:0.003061]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 3018/4300 [01:23<00:26, 47.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69 Batch 42/43 [D loss: 0.005532] [G loss:0.002878]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 3060/4300 [01:24<00:27, 45.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70 Batch 42/43 [D loss: 0.006670] [G loss:0.005599]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 3102/4300 [01:25<00:28, 42.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71 Batch 42/43 [D loss: 0.027787] [G loss:0.000936]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 3146/4300 [01:26<00:23, 48.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72 Batch 42/43 [D loss: 0.010186] [G loss:0.001020]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 3192/4300 [01:27<00:22, 48.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73 Batch 42/43 [D loss: 0.016870] [G loss:0.002176]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3234/4300 [01:28<00:20, 50.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74 Batch 42/43 [D loss: 0.011254] [G loss:0.001986]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 3276/4300 [01:28<00:22, 46.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75 Batch 42/43 [D loss: 0.006984] [G loss:0.001613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 3318/4300 [01:29<00:19, 51.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76 Batch 42/43 [D loss: 0.021525] [G loss:0.002844]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 3360/4300 [01:30<00:18, 50.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77 Batch 42/43 [D loss: 0.004596] [G loss:0.003093]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 3402/4300 [01:31<00:18, 47.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78 Batch 42/43 [D loss: 0.004992] [G loss:0.003252]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 3447/4300 [01:32<00:18, 46.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79 Batch 42/43 [D loss: 0.005159] [G loss:0.001324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 3492/4300 [01:33<00:19, 42.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80 Batch 42/43 [D loss: 0.004301] [G loss:0.000463]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 3532/4300 [01:34<00:18, 42.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81 Batch 42/43 [D loss: 0.014898] [G loss:0.003117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 3577/4300 [01:35<00:16, 43.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82 Batch 42/43 [D loss: 0.009519] [G loss:0.008465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 3619/4300 [01:36<00:14, 48.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83 Batch 42/43 [D loss: 0.060816] [G loss:0.000929]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 3665/4300 [01:37<00:14, 44.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84 Batch 42/43 [D loss: 0.020545] [G loss:0.000438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 3705/4300 [01:38<00:14, 41.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85 Batch 42/43 [D loss: 0.004313] [G loss:0.001642]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 3751/4300 [01:39<00:11, 47.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86 Batch 42/43 [D loss: 0.017178] [G loss:0.000775]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 3791/4300 [01:40<00:12, 42.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87 Batch 42/43 [D loss: 0.013501] [G loss:0.000388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 3836/4300 [01:41<00:10, 46.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88 Batch 42/43 [D loss: 0.054193] [G loss:0.001116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 3877/4300 [01:42<00:08, 51.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89 Batch 42/43 [D loss: 0.021924] [G loss:0.000926]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 3920/4300 [01:42<00:07, 48.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90 Batch 42/43 [D loss: 0.016698] [G loss:0.001845]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 3965/4300 [01:44<00:07, 43.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91 Batch 42/43 [D loss: 0.007806] [G loss:0.000785]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 4005/4300 [01:45<00:07, 41.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92 Batch 42/43 [D loss: 0.006968] [G loss:0.001878]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 4050/4300 [01:46<00:05, 45.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93 Batch 42/43 [D loss: 0.003999] [G loss:0.001393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 4096/4300 [01:47<00:04, 47.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94 Batch 42/43 [D loss: 0.003796] [G loss:0.002675]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 4139/4300 [01:47<00:03, 50.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95 Batch 42/43 [D loss: 0.008143] [G loss:0.000138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 4177/4300 [01:48<00:02, 43.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96 Batch 42/43 [D loss: 0.035209] [G loss:0.000614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 4222/4300 [01:49<00:01, 42.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97 Batch 42/43 [D loss: 0.003131] [G loss:0.001809]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 4264/4300 [01:50<00:00, 49.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98 Batch 42/43 [D loss: 0.003343] [G loss:0.001124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 4299/4300 [01:51<00:00, 48.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99 Batch 42/43 [D loss: 0.001804] [G loss:0.002016]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "plt.plot(discriminator_loss, label='Discriminator')\n",
        "plt.plot(gan_loss, label='Generator')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "gbDjnU3nTxaw",
        "outputId": "ee11b712-907e-4a3e-d600-a9fa41a14f6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7faa36e42990>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the test set\n",
        "\n",
        "nr_batches_test = np.ceil(x_test.shape[0] // batch_size).astype(np.int32)\n",
        "\n",
        "results =[]\n",
        "\n",
        "for t in range(nr_batches_test +1):    \n",
        "   ran_from = t * batch_size\n",
        "   if ran_from<len(x_test):\n",
        "              #print(ran_from)\n",
        "              ran_to = (t + 1) * batch_size\n",
        "              #print(ran_to)\n",
        "              image_batch = x_test[ran_from:ran_to] \n",
        "              #print(image_batch)          \n",
        "              tmp_rslt = discriminator.predict(x=image_batch,batch_size=4,verbose=0)        \n",
        "              #print(tmp_rslt)\n",
        "              results = np.append(results, tmp_rslt)\n",
        "              #print(results)"
      ],
      "metadata": {
        "id": "EBtu96K_YasF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=np.array(y_test)"
      ],
      "metadata": {
        "id": "I5TWyNlsYo3O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:20,.7f}'.format\n",
        "results_df = pd.concat([pd.DataFrame(results),pd.DataFrame(y_test)], axis=1)\n",
        "results_df.columns = ['results','y_test']\n",
        "print ('Mean score for normal packets :', results_df.loc[results_df['y_test'] == 0, 'results'].mean() )\n",
        "print ('Mean score for anomalous packets :', results_df.loc[results_df['y_test'] == 1, 'results'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGpwnxXzYrm3",
        "outputId": "29234103-95c7-4dfe-ab7f-536d055fe2b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean score for normal packets : 0.9923279612554042\n",
            "Mean score for anomalous packets : 0.9994165698687235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw1uWGT-gN5-",
        "outputId": "173d30a7-138a-4451-a1e9-294bac427fd6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 results               y_test\n",
            "0              0.9992324            0.0000000\n",
            "1              0.9999223            0.0000000\n",
            "2              0.9989641            0.0000000\n",
            "3              0.9989676            0.0000000\n",
            "4              0.9994384            0.0000000\n",
            "5              0.9993750            0.0000000\n",
            "6              0.9946026            0.0000000\n",
            "7              0.9998949            0.0000000\n",
            "8              0.9966737            0.0000000\n",
            "9              0.9993710            0.0000000\n",
            "10             0.9986395            0.0000000\n",
            "11             0.9991969            0.0000000\n",
            "12             0.9992616            0.0000000\n",
            "13             0.9970996            0.0000000\n",
            "14             0.9991128            0.0000000\n",
            "15             0.9992008            0.0000000\n",
            "16             0.9996002            0.0000000\n",
            "17             0.9994681            0.0000000\n",
            "18             0.9987544            0.0000000\n",
            "19             0.9982195            0.0000000\n",
            "20             0.9998597            0.0000000\n",
            "21             0.9986751            0.0000000\n",
            "22             0.9991111            0.0000000\n",
            "23             0.9989983            0.0000000\n",
            "24             0.9989518            0.0000000\n",
            "25             0.9994593            0.0000000\n",
            "26             0.9993901            0.0000000\n",
            "27             0.9988873            0.0000000\n",
            "28             0.9990299            0.0000000\n",
            "29             0.9984791            0.0000000\n",
            "30             0.9993298            0.0000000\n",
            "31             0.9998891            0.0000000\n",
            "32             0.9999042            0.0000000\n",
            "33             0.9992120            0.0000000\n",
            "34             0.9985616            0.0000000\n",
            "35             0.9980915            0.0000000\n",
            "36             0.9990615            0.0000000\n",
            "37             0.9998729            0.0000000\n",
            "38             0.9993497            0.0000000\n",
            "39             0.9987378            0.0000000\n",
            "40             0.9999220            0.0000000\n",
            "41             0.9998927            0.0000000\n",
            "42             0.9995468            0.0000000\n",
            "43             0.9994628            0.0000000\n",
            "44             0.9993386            0.0000000\n",
            "45             0.9991857            0.0000000\n",
            "46             0.9981994            0.0000000\n",
            "47             0.9993656            0.0000000\n",
            "48             0.9984781            0.0000000\n",
            "49             0.9993410            0.0000000\n",
            "50             0.9999044            0.0000000\n",
            "51             0.9994299            0.0000000\n",
            "52             0.9991291            0.0000000\n",
            "53             0.9995787            0.0000000\n",
            "54             0.9995219            0.0000000\n",
            "55             0.9975296            0.0000000\n",
            "56             0.9998800            0.0000000\n",
            "57             0.9998553            0.0000000\n",
            "58             0.9990467            0.0000000\n",
            "59             0.9998779            0.0000000\n",
            "60             0.9961134            0.0000000\n",
            "61             0.9993882            0.0000000\n",
            "62             0.9987785            0.0000000\n",
            "63             0.9999030            0.0000000\n",
            "64             0.9998297            0.0000000\n",
            "65             0.9998947            0.0000000\n",
            "66             0.9989470            0.0000000\n",
            "67             0.9998556            0.0000000\n",
            "68             0.9984632            0.0000000\n",
            "69             0.9990306            0.0000000\n",
            "70             0.9999303            0.0000000\n",
            "71             0.9981008            0.0000000\n",
            "72             0.9995637            0.0000000\n",
            "73             0.9994304            0.0000000\n",
            "74             0.9993137            0.0000000\n",
            "75             0.9994133            0.0000000\n",
            "76             0.9991720            0.0000000\n",
            "77             0.9999163            0.0000000\n",
            "78             0.9968840            0.0000000\n",
            "79             0.9992595            0.0000000\n",
            "80             0.9972804            0.0000000\n",
            "81             0.9999182            0.0000000\n",
            "82             0.9999295            0.0000000\n",
            "83             0.9998960            0.0000000\n",
            "84             0.9991760            0.0000000\n",
            "85             0.9992691            0.0000000\n",
            "86             0.9992445            0.0000000\n",
            "87             0.9999111            0.0000000\n",
            "88             0.9986511            0.0000000\n",
            "89             0.9985546            0.0000000\n",
            "90             0.9989169            0.0000000\n",
            "91             0.9992354            0.0000000\n",
            "92             0.9991693            0.0000000\n",
            "93             0.9988899            0.0000000\n",
            "94             0.9988837            0.0000000\n",
            "95             0.9998691            0.0000000\n",
            "96             0.9990077            0.0000000\n",
            "97             0.9992293            0.0000000\n",
            "98             0.9977132            0.0000000\n",
            "99             0.9991680            0.0000000\n",
            "100            0.9974910            0.0000000\n",
            "101            0.9991909            0.0000000\n",
            "102            0.9990761            0.0000000\n",
            "103            0.9992266            0.0000000\n",
            "104            0.9992415            0.0000000\n",
            "105            0.9998935            0.0000000\n",
            "106            0.9970893            0.0000000\n",
            "107            0.9990556            0.0000000\n",
            "108            0.9993955            0.0000000\n",
            "109            0.9992784            0.0000000\n",
            "110            0.9988382            0.0000000\n",
            "111            0.9999001            0.0000000\n",
            "112            0.9990035            0.0000000\n",
            "113            0.9991501            0.0000000\n",
            "114            0.9999287            0.0000000\n",
            "115            0.9995042            0.0000000\n",
            "116            0.9998513            0.0000000\n",
            "117            0.9983271            0.0000000\n",
            "118            0.9991420            0.0000000\n",
            "119            0.9998363            0.0000000\n",
            "120            0.9991118            0.0000000\n",
            "121            0.9951425            0.0000000\n",
            "122            0.9986344            0.0000000\n",
            "123            0.9993800            0.0000000\n",
            "124            0.9987719            0.0000000\n",
            "125            0.9998709            0.0000000\n",
            "126            0.9998474            0.0000000\n",
            "127            0.9998770            0.0000000\n",
            "128            0.9990650            0.0000000\n",
            "129            0.9998571            0.0000000\n",
            "130            0.9989675            0.0000000\n",
            "131            0.9991140            0.0000000\n",
            "132            0.9999384            0.0000000\n",
            "133            0.9986479            0.0000000\n",
            "134            0.9993316            0.0000000\n",
            "135            0.9999356            0.0000000\n",
            "136            0.9998614            0.0000000\n",
            "137            0.9999405            0.0000000\n",
            "138            0.9987456            0.0000000\n",
            "139            0.9999230            0.0000000\n",
            "140            0.9992034            0.0000000\n",
            "141            0.9996107            0.0000000\n",
            "142            0.9994493            0.0000000\n",
            "143            0.9993421            0.0000000\n",
            "144            0.9995282            0.0000000\n",
            "145            0.9991825            0.0000000\n",
            "146            0.9985795            0.0000000\n",
            "147            0.9994140            1.0000000\n",
            "148            0.9992478            0.0000000\n",
            "149            0.9989439            0.0000000\n",
            "150            0.9982827            0.0000000\n",
            "151            0.9991753            0.0000000\n",
            "152            0.9994752            0.0000000\n",
            "153            0.9991425            0.0000000\n",
            "154            0.9993634            0.0000000\n",
            "155            0.9988898            0.0000000\n",
            "156            0.9999051            0.0000000\n",
            "157            0.9987732            0.0000000\n",
            "158            0.9993112            0.0000000\n",
            "159            0.9992230            0.0000000\n",
            "160            0.9993787            0.0000000\n",
            "161            0.9995728            0.0000000\n",
            "162            0.9993688            0.0000000\n",
            "163            0.9998844            0.0000000\n",
            "164            0.9998885            0.0000000\n",
            "165            0.9983863            0.0000000\n",
            "166            0.9995314            0.0000000\n",
            "167            0.9989460            0.0000000\n",
            "168            0.9999388            0.0000000\n",
            "169            0.9990345            0.0000000\n",
            "170            0.9993671            0.0000000\n",
            "171            0.9992514            0.0000000\n",
            "172            0.9991295            0.0000000\n",
            "173            0.9977670            0.0000000\n",
            "174            0.9989001            0.0000000\n",
            "175            0.9994336            0.0000000\n",
            "176            0.9993392            0.0000000\n",
            "177            0.9993286            0.0000000\n",
            "178            0.9998976            0.0000000\n",
            "179            0.9992152            0.0000000\n",
            "180            0.9987576            0.0000000\n",
            "181            0.9991801            0.0000000\n",
            "182            0.9999161            0.0000000\n",
            "183            0.9999223            0.0000000\n",
            "184            0.9992556            0.0000000\n",
            "185            0.9968797            0.0000000\n",
            "186            0.9994960            0.0000000\n",
            "187            0.9995264            0.0000000\n",
            "188            0.9992467            0.0000000\n",
            "189            0.9991775            0.0000000\n",
            "190            0.9999132            0.0000000\n",
            "191            0.9998872            0.0000000\n",
            "192            0.9992499            0.0000000\n",
            "193            0.9988626            0.0000000\n",
            "194            0.9990461            0.0000000\n",
            "195            0.9986657            0.0000000\n",
            "196            0.9998115            0.0000000\n",
            "197            0.9985419            0.0000000\n",
            "198            0.9981922            0.0000000\n",
            "199            0.9987603            0.0000000\n",
            "200            0.9989530            0.0000000\n",
            "201            0.9995369            0.0000000\n",
            "202            0.9991817            0.0000000\n",
            "203            0.9986171            0.0000000\n",
            "204            0.9986618            0.0000000\n",
            "205            0.9987342            0.0000000\n",
            "206            0.9990563            0.0000000\n",
            "207            0.9985331            0.0000000\n",
            "208            0.9957997            0.0000000\n",
            "209            0.9990767            0.0000000\n",
            "210            0.9999066            0.0000000\n",
            "211            0.9963628            0.0000000\n",
            "212            0.9999052            0.0000000\n",
            "213            0.9987714            0.0000000\n",
            "214            0.9998872            0.0000000\n",
            "215            0.9974257            0.0000000\n",
            "216            0.9987091            0.0000000\n",
            "217            0.9995222            0.0000000\n",
            "218            0.9992915            0.0000000\n",
            "219            0.9993902            0.0000000\n",
            "220            0.9966305            0.0000000\n",
            "221            0.9998950            0.0000000\n",
            "222            0.9969918            0.0000000\n",
            "223            0.9991691            0.0000000\n",
            "224            0.9989519            0.0000000\n",
            "225            0.9999295            0.0000000\n",
            "226            0.9994118            0.0000000\n",
            "227            0.9999064            0.0000000\n",
            "228            0.9994620            0.0000000\n",
            "229            0.9998682            0.0000000\n",
            "230            0.9994437            0.0000000\n",
            "231            0.9991934            1.0000000\n",
            "232            0.9999055            0.0000000\n",
            "233            0.9991848            0.0000000\n",
            "234            0.9999081            0.0000000\n",
            "235            0.9994234            0.0000000\n",
            "236            0.9982234            0.0000000\n",
            "237            0.9999121            0.0000000\n",
            "238            0.9994391            0.0000000\n",
            "239            0.9992483            0.0000000\n",
            "240            0.9991024            0.0000000\n",
            "241            0.9994515            0.0000000\n",
            "242            0.9970047            0.0000000\n",
            "243            0.9992161            0.0000000\n",
            "244            0.9998950            0.0000000\n",
            "245            0.9987154            0.0000000\n",
            "246            0.9998891            0.0000000\n",
            "247            0.9988707            0.0000000\n",
            "248            0.9990793            0.0000000\n",
            "249            0.9991302            0.0000000\n",
            "250            0.9942222            0.0000000\n",
            "251            0.9993044            0.0000000\n",
            "252            0.9964896            0.0000000\n",
            "253            0.9998869            0.0000000\n",
            "254            0.9992311            0.0000000\n",
            "255            0.9992902            0.0000000\n",
            "256            0.9993846            0.0000000\n",
            "257            0.9998562            0.0000000\n",
            "258            0.9999151            0.0000000\n",
            "259            0.9988467            0.0000000\n",
            "260            0.9994311            0.0000000\n",
            "261            0.9998931            0.0000000\n",
            "262            0.9988953            0.0000000\n",
            "263            0.9988987            0.0000000\n",
            "264            0.9997891            0.0000000\n",
            "265            0.9988662            0.0000000\n",
            "266            0.9951092            0.0000000\n",
            "267            0.9988343            0.0000000\n",
            "268            0.9987332            0.0000000\n",
            "269            0.9993916            0.0000000\n",
            "270            0.9984241            0.0000000\n",
            "271            0.9991102            0.0000000\n",
            "272            0.9998188            0.0000000\n",
            "273            0.9994603            0.0000000\n",
            "274            0.0000000            0.0000000\n",
            "275            0.9999527            0.0000000\n",
            "276            0.9999445            0.0000000\n",
            "277            0.9990089            0.0000000\n",
            "278            0.9991089            0.0000000\n",
            "279            0.9974583            0.0000000\n",
            "280            0.9999214            0.0000000\n",
            "281            0.9999168            0.0000000\n",
            "282            0.9996423            1.0000000\n",
            "283            0.9993810            0.0000000\n",
            "284            0.9990327            0.0000000\n",
            "285            0.9975317            0.0000000\n",
            "286            0.9998164            0.0000000\n",
            "287            0.9994068            0.0000000\n",
            "288            0.9986874            0.0000000\n",
            "289            0.9994494            0.0000000\n",
            "290            0.9992363            0.0000000\n",
            "291            0.9990851            0.0000000\n",
            "292            0.9961541            0.0000000\n",
            "293            0.9992470            0.0000000\n",
            "294            0.0000000            0.0000000\n",
            "295            0.9986929            0.0000000\n",
            "296            0.9984258            0.0000000\n",
            "297            0.9992648            0.0000000\n",
            "298            0.9970628            0.0000000\n",
            "299            0.9976603            0.0000000\n"
          ]
        }
      ]
    }
  ]
}